{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "milestone1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "zwcOPgPjvWY5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c08df087-30b0-415b-8f06-9e20d9e43129"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation , Embedding , Flatten\n",
        "from keras.callbacks import Callback\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "# right now we only take 10k samples to make the testing of the code faster\n",
        "sample_size = 10000\n",
        "# this is the maximum number of words we take from the blogs\n",
        "max_length = 500\n",
        "df = pd.read_json(\"data.json\")\n",
        "df.head()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>post</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27</td>\n",
              "      <td>male</td>\n",
              "      <td>Thabo admits defeat on quiet diplomacy  Mbeki ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>25</td>\n",
              "      <td>male</td>\n",
              "      <td>Brainbench welcomes its 5 millionth subscriber...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>23</td>\n",
              "      <td>female</td>\n",
              "      <td>Even though the air in Jerusalem is dry, it is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25</td>\n",
              "      <td>female</td>\n",
              "      <td>there's nothing else more embarassing in life ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>38</td>\n",
              "      <td>female</td>\n",
              "      <td>Today I had a glass artist over for a firing. ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  gender                                               post\n",
              "0   27    male  Thabo admits defeat on quiet diplomacy  Mbeki ...\n",
              "1   25    male  Brainbench welcomes its 5 millionth subscriber...\n",
              "2   23  female  Even though the air in Jerusalem is dry, it is...\n",
              "3   25  female  there's nothing else more embarassing in life ...\n",
              "4   38  female  Today I had a glass artist over for a firing. ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "I6emCuYdTDQA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e0dd02c6-cc98-45af-e04a-1dcbe34ec1e1"
      },
      "cell_type": "code",
      "source": [
        "# We utilized nltk module to remove known stopwords from the blog texts.\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "  \n",
        "stop_words = set(stopwords.words('english'))    #set of stopwords\n",
        "print(stop_words)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "{'all', \"haven't\", 'there', 'he', 'other', 'mightn', 'those', 'shouldn', 'an', 'hasn', 'under', 'do', 'its', 'when', 'where', 'her', 'hadn', \"wouldn't\", 'if', 'his', 'than', 'is', 'for', 'can', 'at', 'again', 'as', 'being', 'yourself', 'until', 'each', 'i', 'but', 'down', 'am', \"you're\", 'too', 'further', 'before', 'theirs', 'what', 'didn', 'on', 'very', 'should', 'she', 'after', 'm', 'only', 'while', \"should've\", \"hasn't\", \"mightn't\", 'wouldn', 'up', 'were', 'in', 'into', 'needn', 'with', 'same', 'how', \"wasn't\", 'your', 'doing', 'just', \"didn't\", 'o', 'against', 'me', 'you', 'been', 'aren', 'nor', 'whom', 's', 'this', 'that', \"weren't\", 'above', 'here', 'himself', 'ours', 'most', \"that'll\", \"shouldn't\", 'won', 'the', 'which', 'y', 'through', 'shan', 'him', 'such', 'have', 'who', 'from', 'or', 'so', 'll', 'any', \"doesn't\", 'once', 'ourselves', 'mustn', 'did', 'd', 'not', 'will', 'wasn', 'no', 'isn', 'about', 'why', \"aren't\", 'does', 'by', \"couldn't\", \"won't\", 'itself', 'to', 'yourselves', 'weren', 'herself', 'couldn', 'ain', 'off', \"she's\", 'them', 'more', 'yours', 'our', 'we', 'and', 'some', 'below', 'doesn', 'a', 'my', \"mustn't\", 't', 'it', \"shan't\", \"you've\", 'having', 'few', \"needn't\", 'had', 'because', 'these', 'their', 'of', \"you'd\", 'hers', \"it's\", 'was', 'be', \"don't\", \"you'll\", 'between', 'they', 'has', 'ma', 'own', 'themselves', 'out', 'both', 'haven', 'are', 'during', 'myself', 're', \"hadn't\", \"isn't\", 'don', 'then', 'over', 've', 'now'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aXkuP4s6ndeK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1377d43e-ca93-408b-8674-d2c5af5e26f7"
      },
      "cell_type": "code",
      "source": [
        "# get sample_size amount of data from the database\n",
        "values = df.values[0:sample_size]\n",
        "print(values.shape)\n",
        "#print(values[0:2])\n",
        "\n",
        "docs = values[: , 2]\n",
        "#print(docs.shape)\n",
        "labels = values[: , 0:2]\n",
        "#print(docs[0:5])\n",
        "#print(labels[0:5][:])\n",
        "\n",
        "#change database to lower case letters\n",
        "for i,doc in enumerate(docs):\n",
        "  docs[i] = doc.lower()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TK1rp4I83XFy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "18e34ab5-53da-4912-d51e-6f19a18a1fc4"
      },
      "cell_type": "code",
      "source": [
        "#WORD DICTIONARY START\n",
        "# Here we create a word dictionary of the blog posts, to see how many different\n",
        "# words are in them, to decide what the vocab_size should be.\n",
        "# We also check how many words we might not even need, because they appear very\n",
        "# infrequently.\n",
        "# We also use this to create a list of the infrequent words, so we can remove them,\n",
        "# to see if it helps later in the modelling phase.\n",
        "import re\n",
        "\n",
        "docs_to_process = docs\n",
        "\n",
        "  \n",
        "def get_word_dict(docs_to_process):\n",
        "    word_dict = {}\n",
        "    for text in docs_to_process:\n",
        "        words = re.findall(r\"[\\w']+\", text)\n",
        "        for word in words:\n",
        "            if word in word_dict:\n",
        "                word_dict[word] = word_dict[word] + 1\n",
        "            else:\n",
        "                word_dict[word] = 1\n",
        "      \n",
        "    return word_dict\n",
        "  \n",
        "  \n",
        "word_dict = get_word_dict(docs_to_process)\n",
        "\n",
        "infrequent_words = []\n",
        "\n",
        "word_dict_small = word_dict.copy()\n",
        "for elem in word_dict:\n",
        "    if word_dict[elem] <= 1:\n",
        "      infrequent_words.append(elem)\n",
        "      del word_dict_small[elem]\n",
        "      \n",
        "print(\"Size of the word dictionary: \" + str(len(word_dict)))\n",
        "print(\"Size without infrequent words: \" + str(len(word_dict_small)))\n",
        "#print(infrequent_words)\n",
        "\n",
        "# Create a set that contains everything to remove\n",
        "#rm_words = set(infrequent_words)\n",
        "#print(rm_words)\n",
        "\n",
        "#WORD DICTIONARY STOP"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the word dictionary: 68942\n",
            "Size without infrequent words: 34032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "esHeQYK0XdWt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This function removes stopwords and also words that appear infrequently, and it cuts blogposts longer than 500 words.\n",
        "\n",
        "def reduce_vocab(docs, word_limit, sparsewords=False):\n",
        "  for i,  blogpost in enumerate(docs):\n",
        "    word_tokens = word_tokenize(docs[i])\n",
        "    \n",
        "    if(len(word_tokens) > word_limit):\n",
        "      word_tokens = word_tokens[0:word_limit]\n",
        "    \n",
        "    blogpost_reduced = [w for w in word_tokens if not w in stop_words]\n",
        "    \n",
        "    if(sparsewords == True):\n",
        "      blogpost_reduced = [w for w in blogpost_reduced if not w in infrequent_words ]\n",
        "      \n",
        "    docs[i] = ' '.join( blogpost_reduced ) #.replace(' , ',',').replace(' .','.').replace(' !','!').replace(' ?','?')\n",
        "    \n",
        "# Right now the removal of sparse/infrequent words is slow, we might have to find a different way to reduce vocabulary size\n",
        "reduce_vocab(docs, max_length, sparsewords=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cEnAoedIZnRk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "60596938-718d-4a74-f8e6-ebb757a7e6de"
      },
      "cell_type": "code",
      "source": [
        "#testing how the vocabulary reduction worked by examples:\n",
        "df_orig = pd.read_json(\"data.json\")\n",
        "docs_orig = df_orig.values[:,2]\n",
        "\n",
        "print(docs_orig[0])\n",
        "print(docs[0])\n",
        "print(docs_orig[1])\n",
        "print(docs[1])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thabo admits defeat on quiet diplomacy  Mbeki  urlLink stated  yesterday that his policy of quiet diplomacy on Zimbabwe has been a failure. Speaking through presidential spokesperson Bheki Khumalo, he said that the talks between the Zanu-PF and MDC were 'too slow' and that no progress had been made on the issue.  However, Mbeki also stated that he would \"press on with his diplomatic efforts in Zimbabwe despite fierce criticism, because he still believed there was no alternative to dialogue.\" These is a confusing sentiment. There are many political alternatives to dialogue that include simple public censure and economic pressure.  Mbeki has largely based himself as a foreign policy president, particularly in his first term, and to gain any credibility for NEPAD and his other strong foreign policy initiatives, he must take action on Zimbabwe. After his softening position on AIDS, it is the one factor in his presidency that the world still cannot understand. Struggle friends or not, a dictator is a dictator whatever the historical context, and Mbeki has to show his leadership and take action after this acknolwedgement. Will it happen? Probably not...\n",
            "thabo admits defeat quiet diplomacy mbeki urllink stated yesterday policy quiet diplomacy zimbabwe failure . speaking presidential spokesperson bheki khumalo , said talks zanu-pf mdc 'too slow ' progress made issue . however , mbeki also stated would `` press diplomatic efforts zimbabwe despite fierce criticism , still believed alternative dialogue . '' confusing sentiment . many political alternatives dialogue include simple public censure economic pressure . mbeki largely based foreign policy president , particularly first term , gain credibility nepad strong foreign policy initiatives , must take action zimbabwe . softening position aids , one factor presidency world still understand . struggle friends , dictator dictator whatever historical context , mbeki show leadership take action acknolwedgement . happen ? probably ...\n",
            "Brainbench welcomes its 5 millionth subscriber by announcing free tests from 1st of july through 14th of july 2004. Well.. it really sounds like MUSIC and that too ROCK MUSIC for the ears of brainbench certification aspirants but wait..... all has got jazzed up. What happens when an apple needs to be shared between 1000s of hungry men? of course everyone is left without a piece of it. Thats what exactly has happned with Brainbench. After announcing the free tests they did not take care for the high availability of the data through their servers. For more than half an hour i tried to just login, forget taking any test.  By the way how much difference does it make if you get these certifications. I and may be the community would like to know any success stories. Please share your comments.  [Resource-Type: Views; Category: General]\n",
            "brainbench welcomes 5 millionth subscriber announcing free tests 1st july 14th july 2004. well.. really sounds like music rock music ears brainbench certification aspirants wait ... .. got jazzed . happens apple needs shared 1000s hungry men ? course everyone left without piece . thats exactly happned brainbench . announcing free tests take care high availability data servers . half hour tried login , forget taking test . way much difference make get certifications . may community would like know success stories . please share comments . [ resource-type : views ; category : general ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "doBaIK8uulJ8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataout = {'age' : labels[:,0] , 'gender': labels[:,1] , 'post': docs}\n",
        "\n",
        "dfout = pd.DataFrame( data=dataout ) \n",
        "dfout.to_json('out.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Txp9ep10YTZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ce6deba-b217-45e1-f70e-98306686503d"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "df_processed = pd.read_json(\"out.json\")\n",
        "df_processed.head()\n",
        "\n",
        "values_processed = df_processed.values\n",
        "docs_processed = values_processed[:,2]\n",
        "labels_processed = values_processed[: , 0:2]\n",
        "# check finalized word_dict:\n",
        "final_wd = get_word_dict(docs_processed)\n",
        "print(\"final vocabulary size is: \" + str(len(final_wd)))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final vocabulary size is: 59148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CmrEcSNyzUwY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36eb743c-1119-4c97-ae63-31e1402a26b5"
      },
      "cell_type": "code",
      "source": [
        "# We chose a vocab size based on the word dictionary's length\n",
        "vocab_size = len(final_wd)\n",
        "encoded_docs = [keras.preprocessing.text.one_hot(d, vocab_size , filters='') for d in docs_processed]\n",
        "#print(\"One hot encoded docs: \" , encoded_docs)\n",
        "#print(\"Count of docs: \",len(encoded_docs))\n",
        "#print(\"Length of the first doc after one-hot:\", len(encoded_docs[0]))      \n",
        "#print(\"Lenght of the original doc in words:\" , len(docs2[0].split(' ')))\n",
        "# According to the documentation the difference is because keras' one-hot removes special characters.\n",
        "\n",
        "lengths= []\n",
        "lenmax = 0\n",
        "for i in range(len(encoded_docs)):\n",
        "  lengths.append( len(docs[i].split(' ')))\n",
        "  if ( lengths[-1] > lenmax ):\n",
        "      lenmax = lengths[-1]\n",
        "print(\"length of blog post containing most words is:\" , lenmax )\n"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of blog post containing most words is: 497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KBSl3wcDT9lM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47fe733e-3672-4cba-f0d6-de265610a1d0"
      },
      "cell_type": "code",
      "source": [
        "labels_processed.shape"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "metadata": {
        "id": "gmI836J7z4DO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "be66d660-0f8d-4306-a19e-a1e048926c3d"
      },
      "cell_type": "code",
      "source": [
        "# This is where the input vectors get padded to the same size. The length has to be longest of all the one-hot encoded inputs.\n",
        "\n",
        "padded_docs = keras.preprocessing.sequence.pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(len(padded_docs[0]))    #check the lenght of a padded vector"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U_WAe5Jvl_Hl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get valid/test/train split\n",
        "valid_split = 0.2\n",
        "test_split = 0.1\n",
        "nb_samples = values.shape[0]\n",
        "\n",
        "\n",
        "X_train = padded_docs[0:int(nb_samples*(1-valid_split-test_split))]\n",
        "Y_train = labels_processed[0:int(nb_samples*(1-valid_split-test_split)),:]\n",
        "X_valid = padded_docs[int(nb_samples*(1-valid_split-test_split)):int(nb_samples*(1-test_split))]\n",
        "Y_valid = labels_processed[int(nb_samples*(1-valid_split-test_split)):int(nb_samples*(1-test_split)),:]\n",
        "X_test  = padded_docs[int(nb_samples*(1-test_split)):]\n",
        "Y_test  = labels_processed[int(nb_samples*(1-test_split)):,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iPC75ReZIetr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "93665c4d-0056-4e25-d80b-c34b0298e8bd"
      },
      "cell_type": "code",
      "source": [
        "# check shapes\n",
        "print(\"X_train shape: \" + str(X_train.shape))\n",
        "print(\"Y_train shape: \" + str(Y_train.shape))\n",
        "print(\"X_valid shape: \" + str(X_valid.shape))\n",
        "print(\"Y_valid shape: \" + str(Y_valid.shape))\n",
        "print(\"X_test shape: \" + str(X_test.shape))\n",
        "print(\"Y_test shape: \" + str(Y_test.shape))"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (7000, 500)\n",
            "Y_train shape: (7000, 2)\n",
            "X_valid shape: (2000, 500)\n",
            "Y_valid shape: (2000, 2)\n",
            "X_test shape: (1000, 500)\n",
            "Y_test shape: (1000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B21p32frIhUL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}